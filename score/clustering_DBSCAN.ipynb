{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCANのeps設定のための描画\n",
    "- あんまりわかってない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "def get_kdist_plot(X=None, k=None, radius_nbrs=1.0):\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, radius=radius_nbrs).fit(X)\n",
    "\n",
    "    # For each point, compute distances to its k-nearest neighbors\n",
    "    distances, indices = nbrs.kneighbors(X) \n",
    "\n",
    "    distances = np.sort(distances, axis=0)\n",
    "    distances = distances[:, k-1]\n",
    "\n",
    "    # Plot the sorted K-nearest neighbor distance for each point in the dataset\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.plot(distances)\n",
    "    plt.xlabel('Points/Objects in the dataset', fontsize=12)\n",
    "    plt.ylabel('Sorted {}-nearest neighbor distance'.format(k), fontsize=12)\n",
    "    plt.grid(True, linestyle=\"--\", color='black', alpha=0.4)\n",
    "    plt.ylim((0,0.2))\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def readCSV(file_path, parse_func) -> list[list]:\n",
    "    data = None\n",
    "    with open(file_path, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        content = [row for row in reader]\n",
    "        data:list[list] = []\n",
    "        for row in content:\n",
    "            row_data = []\n",
    "            for d in row:\n",
    "                row_data.append(parse_func(d))\n",
    "            data.append(row_data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## クラスタリングの評価\n",
    "- return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,multilabel_confusion_matrix,precision_score,recall_score,f1_score\n",
    "\n",
    "def acc_est(clustering_result, quiet=False):\n",
    "    return _est(clustering_result, \"./data/note_sims/sim_acc.csv\", \"./data/note_sims/not_sim_acc.csv\", quiet)\n",
    "\n",
    "def est(clustering_result, quiet=False):\n",
    "    return _est(clustering_result, \"./data/note_sims/sim.csv\", \"./data/note_sims/not_sim.csv\", quiet)\n",
    "\n",
    "def label_est(clustering_result, lengths_by_id):\n",
    "    count = len(clustering_result)\n",
    "    labels = {True: 1, False:0, None:\"none\"}\n",
    "    result_table = [[labels[i == j] for j in clustering_result] for i in clustering_result]\n",
    "    sim_table = [[labels[None] for _ in clustering_result] for _ in clustering_result]\n",
    "    sim_label_data = open_sim_label_file(\"./data/note_sims/sim_label_318.csv\")\n",
    "    for [s,t, label] in sim_label_data:\n",
    "        # print(s+lengths_by_id[155],t+lengths_by_id[155])\n",
    "        sim_table[s+lengths_by_id[155]][t+lengths_by_id[155]] = labels[True]\n",
    "        sim_table[t+lengths_by_id[155]][s+lengths_by_id[155]] = labels[True]\n",
    "    \n",
    "    sims_true = []\n",
    "    sims = []\n",
    "    for i in range(count):\n",
    "        for j in range(i+1,count):\n",
    "            if sim_table[i][j] == labels[None]:\n",
    "                continue\n",
    "            \n",
    "            sims_true.append(sim_table[i][j])\n",
    "            sims.append(result_table[i][j])\n",
    "\n",
    "    confusion_matrix = multilabel_confusion_matrix(sims_true, sims)\n",
    "    accuracy = accuracy_score(sims_true, sims)\n",
    "    precision = precision_score(sims_true, sims)\n",
    "    recall = recall_score(sims_true, sims)\n",
    "    f1 = f1_score(sims_true, sims)\n",
    "    # print(f\"accuracy = {accuracy_score()}\")\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def open_sim_label_file(file_path):\n",
    "    sim_data_label = None\n",
    "    with open(file_path, newline=\"\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        content = [row for row in reader]\n",
    "        header = content[0]\n",
    "        sim_data_label = [[int(row[0]), int(row[1]), row[2]] for row in content[1:]]\n",
    "    return sim_data_label\n",
    "\n",
    "def _est(clustering_result,sim_file_path, nsim_file_path, quiet=False):\n",
    "    count = len(clustering_result)\n",
    "    labels = {True: 1, False:0, None:\"none\"}\n",
    "    result_table = [[labels[i == j] for j in clustering_result] for i in clustering_result]\n",
    "    sim_table = [[labels[None] for _ in clustering_result] for _ in clustering_result]\n",
    "    sim_data = readCSV(sim_file_path, int)\n",
    "    not_sim_data = readCSV(nsim_file_path, int)\n",
    "    if not sim_data or sim_data is None or len(sim_data) == 0:\n",
    "        print(\"load error\")\n",
    "        return\n",
    "    if not not_sim_data or not_sim_data is None or len(not_sim_data) == 0:\n",
    "        print(\"load error\")\n",
    "        return\n",
    "\n",
    "    for [s,t] in sim_data:\n",
    "        sim_table[s][t] = labels[True]\n",
    "        sim_table[t][s] = labels[True]\n",
    "    for [s,t] in not_sim_data:\n",
    "        sim_table[s][t] = labels[False]\n",
    "        sim_table[t][s] = labels[False]\n",
    "\n",
    "    sims_true = []\n",
    "    sims = []\n",
    "    for i in range(count):\n",
    "        for j in range(i+1,count):\n",
    "            if sim_table[i][j] == labels[None]:\n",
    "                continue\n",
    "            \n",
    "            sims_true.append(sim_table[i][j])\n",
    "            sims.append(result_table[i][j])\n",
    "\n",
    "    confusion_matrix = multilabel_confusion_matrix(sims_true, sims)\n",
    "    accuracy = accuracy_score(sims_true, sims)\n",
    "    precision = precision_score(sims_true, sims)\n",
    "    recall = recall_score(sims_true, sims)\n",
    "    f1 = f1_score(sims_true, sims)\n",
    "    \n",
    "    if not quiet:\n",
    "        print(\"\\n===== 評価 =====\")\n",
    "        print(np.array([[\"tp rate\", \"fn rate\"], [\"tn rate\", \"tn rate\"]]))\n",
    "        for label, matrix in zip(labels.values(), confusion_matrix):\n",
    "            print(f\"{label=}\")\n",
    "            sm = sum(sum(matrix))\n",
    "            print(f\"{matrix/sm}\")\n",
    "        print(f\"{accuracy=}\")\n",
    "        print(f\"{precision=}\")\n",
    "        print(f\"{recall=}\")\n",
    "        print(f\"{f1=}\")\n",
    "        print(\"=================\\n\")\n",
    "    # print(f\"accuracy = {accuracy_score()}\")\n",
    "    return accuracy, precision, recall, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import csv\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "notes_file_paths = glob.glob(\"./data/_json/feature_vector/test/*song155*[[]test[]]*.json\")\n",
    "notes_file_paths += glob.glob(\"./data/_json/feature_vector/test/*song318*[[]test[]]*.json\")\n",
    "# notes_file_paths = [f for f in notes_file_paths if \"155\" in f or \"318\" in f]\n",
    "print(notes_file_paths)\n",
    "data = []\n",
    "ids = []\n",
    "lengths = []\n",
    "lengths_by_id = dict()\n",
    "id_by_path = dict()\n",
    "for file_path in notes_file_paths:\n",
    "    # data = None\n",
    "    # id = None\n",
    "    with open(file_path, newline=\"\") as f:\n",
    "        content = json.load(f)\n",
    "        data += content[\"data\"]\n",
    "        ids.append(content[\"id\"])\n",
    "        lengths.append(len(content[\"data\"]))\n",
    "        lengths_by_id[int(content[\"id\"])] = len(content[\"data\"])\n",
    "        id_by_path[file_path] = content[\"id\"]\n",
    "        # print(\"data len\", len(data))\n",
    "        # print(\"content len\", len(content[\"data\"]))\n",
    "        # print(lengths_by_id)\n",
    "\n",
    "    # k = 2 * len(data[0]) - 1 # k=2*{dim(dataset)} - 1\n",
    "    # get_kdist_plot(X=data, k=k)\n",
    "data = preprocessing.StandardScaler().fit_transform(np.array(data))\n",
    "# train_data = preprocessing.StandardScaler().fit_transform(np.array(train_data))\n",
    "# クラスタリング\n",
    "range_max = 1\n",
    "range_step = 10\n",
    "range_min = range_max/range_step\n",
    "step = range_max/range_step\n",
    "now = range_min\n",
    "rng = []\n",
    "for i in range(range_step):\n",
    "    if now < 1:\n",
    "        rng.append(now)\n",
    "    now += step\n",
    "\n",
    "\n",
    "epses = []\n",
    "silhouettes = []\n",
    "cluster_counts = []\n",
    "weight_scores = []\n",
    "no_weight_scores = []\n",
    "score_318 = []\n",
    "\n",
    "for eps in rng:\n",
    "    print(f\"eps = {eps}\")\n",
    "    epses.append(eps)\n",
    "\n",
    "    clustering_result = DBSCAN(eps=eps, min_samples=3).fit_predict(data)\n",
    "    clustering_result += 1\n",
    "    # noise = len(set(clustering_result))+2\n",
    "    # for i in range(len(clustering_result)):\n",
    "    #     if clustering_result[i] == -1:\n",
    "    #         clustering_result[i] = noise\n",
    "    #         noise += 1\n",
    "    \n",
    "\n",
    "    cluster_count = len(set(clustering_result))\n",
    "    cluster_counts.append(cluster_count)\n",
    "    print(f\"clusters count: {cluster_count}, length: {len(clustering_result)}\")\n",
    "\n",
    "    silhouette = silhouette_score(data, clustering_result)\n",
    "    silhouettes.append(silhouette)\n",
    "\n",
    "    accuracy, precision, recall, f1 = acc_est(clustering_result, True)\n",
    "    weight_scores.append([accuracy, precision, recall, f1])\n",
    "\n",
    "    accuracy, precision, recall, f1 = est(clustering_result, True)\n",
    "    no_weight_scores.append([accuracy, precision, recall, f1])\n",
    "    le = label_est(clustering_result, lengths_by_id)\n",
    "    score_318.append(list(le))\n",
    "\n",
    "\n",
    "def print_scores_max(scores):\n",
    "    accuracy_max = max(scores[0])\n",
    "    recall_max = max(scores[1])\n",
    "    precision_max = max(scores[2])\n",
    "    recall_max = max(scores[3])\n",
    "    print(f\"{accuracy_max=}\\n{recall_max}\\n{precision_max}\\n{recall_max}\")\n",
    "\n",
    "\n",
    "weight_scores = list(zip(*weight_scores))\n",
    "no_weight_scores = list(zip(*no_weight_scores))\n",
    "score_318 = list(zip(*score_318))\n",
    "print_scores_max(weight_scores)\n",
    "print_scores_max(no_weight_scores)\n",
    "print_scores_max(score_318)\n",
    "\n",
    "\n",
    "plt.plot(epses, silhouettes)\n",
    "plt.title(\"silhouette\")\n",
    "plt.ylim([-1.05, 1.05])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(epses, cluster_counts)\n",
    "plt.title(\"cluster number\")\n",
    "plt.ylim(bottom=-0.05)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "colors = [\"red\", \"blue\", \"green\", \"cyan\"]\n",
    "labels = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
    "\n",
    "\n",
    "# weight score\n",
    "for i in range(len(weight_scores)):\n",
    "    plt.plot(epses, weight_scores[i], color=colors[i], label=labels[i])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel(\"eps\")\n",
    "plt.title(\"weight\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# no_weight_scores\n",
    "for i in range(len(no_weight_scores)):\n",
    "    plt.plot(epses, no_weight_scores[i], color=colors[i], label=labels[i])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel(\"eps\")\n",
    "plt.title(\"no weight\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for i in range(len(score_318)):\n",
    "    plt.plot(epses, score_318[i], color=colors[i], label=labels[i])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel(\"eps\")\n",
    "plt.title(\"score_318\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特定のepsを描画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "import csv\n",
    "\n",
    "def clustering(data, eps):\n",
    "    clustering_result = DBSCAN(eps=eps, min_samples=3).fit_predict(data)\n",
    "    noise = len(set(clustering_result))+2\n",
    "    for i in range(len(clustering_result)):\n",
    "        if clustering_result[i] == -1:\n",
    "            clustering_result[i] = -1*noise\n",
    "            noise += 1\n",
    "    return clustering_result\n",
    "\n",
    "def dim_reduce_tsne(data):\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    data_tsne = tsne.fit_transform(data)\n",
    "    return data_tsne\n",
    "\n",
    "def clustering_tsne_plot(data,eps):\n",
    "    data_tsne = dim_reduce_tsne(data)\n",
    "    clustering_result = clustering(data, eps)\n",
    "\n",
    "    plt.scatter(data_tsne[:,0],data_tsne[:,1], c=clustering_result, cmap=\"gist_ncar\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "clustering_result = clustering(data, 0.1)\n",
    "sim_label_data = open_sim_label_file(\"./data/note_sims/sim_label_318.csv\")\n",
    "\n",
    "ids_by_label:dict[str, set[int]] = dict()\n",
    "for [s,t, label] in sim_label_data:\n",
    "    arr = ids_by_label.get(label, set())\n",
    "    arr.add(s+lengths_by_id[155])\n",
    "    arr.add(t+lengths_by_id[155])\n",
    "    ids_by_label[label] = arr\n",
    "\n",
    "colors = [\"red\", \"green\", \"blue\", \"purple\", \"black\", \"cyan\"]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "data_tsne = tsne.fit_transform(data)\n",
    "plt.scatter(data_tsne[:,0],data_tsne[:,1], s=[50]*len(data_tsne),c=clustering_result, cmap=\"gist_ncar\")\n",
    "for i, (k, ids) in enumerate(ids_by_label.items()):\n",
    "    show_data = np.array([d for i,d in enumerate(data_tsne) if i in ids])\n",
    "    plt.scatter(show_data[:,0],show_data[:,1], color=colors[i], s=[8]*len(show_data), label=k)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "from umap import UMAP\n",
    "umap = UMAP(n_components=2, random_state=0)\n",
    "data_umap = umap.fit_transform(data)\n",
    "plt.scatter(data_umap[:,0],data_umap[:,1], c=clustering_result, cmap=\"gist_ncar\")\n",
    "for i, (k, ids) in enumerate(ids_by_label.items()):\n",
    "    show_data = np.array([d for i,d in enumerate(data_umap) if i in ids])\n",
    "    plt.scatter(show_data[:,0],show_data[:,1], color=colors[i], s=[8]*len(show_data), label=k)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2, random_state=0)\n",
    "data_pca = pca.fit_transform(data)\n",
    "plt.scatter(data_pca[:,0],data_pca[:,1], c=clustering_result, cmap=\"gist_ncar\")\n",
    "for i, (k, ids) in enumerate(ids_by_label.items()):\n",
    "    show_data = np.array([d for i,d in enumerate(data_pca) if i in ids])\n",
    "    plt.scatter(show_data[:,0],show_data[:,1], color=colors[i], s=[8]*len(show_data), label=k)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# クラスタリング結果の保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "def save_clustering_tsne_data(data,eps, save_dir, save_file_name):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    with open(f\"{save_dir}/{save_file_name}\", \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"id\",\"x\", \"y\", \"label\"])\n",
    "    clustering_result = clustering(data,eps)\n",
    "    data_tsne = dim_reduce_tsne(data)\n",
    "    # 各区間ごとのデータ\n",
    "    # for d in zip(data_tsne, clustering_result):\n",
    "    #     print(d)\n",
    "    clustering_data = [[id,float(pos[0]), float(pos[1]), int(l)]for (pos, l) in zip(data_tsne, clustering_result)]\n",
    "\n",
    "    with open(f\"{save_dir}/{save_file_name}\", \"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(clustering_data)\n",
    "\n",
    "save_dir = \"./data/_json/0726/clustering_result\"\n",
    "save_file_name = \"clustering_data_test.csv\"\n",
    "save_clustering_tsne_data(data,0.1, save_dir, save_file_name)\n",
    "\n",
    "# base_dir = \"./data/_vis-result\"\n",
    "# time_dir_path = f\"{base_dir}/{str(datetime.date.today())}\"\n",
    "# cluster_labels_dir = f\"{time_dir_path}/{str(datetime.datetime.now().time())}-cluster-labels\"\n",
    "# os.makedirs(cluster_labels_dir, exist_ok=True)\n",
    "\n",
    "# clustering_result = clustering(0.1)\n",
    "# clustering_labels = set(clustering_result)\n",
    "# for label in clustering_labels:\n",
    "#     for id in ids:\n",
    "#         os.makedirs(f\"{cluster_labels_dir}/{id}/{label}\", exist_ok=True)\n",
    "\n",
    "# clustering_result_by_id = dict()\n",
    "# result_idx = 0\n",
    "# for id, length in zip(ids, lengths):\n",
    "#     result = clustering_result[result_idx:result_idx+length]\n",
    "#     clustering_result_by_id[id] = result\n",
    "#     result_idx = length\n",
    "#     print(len(result), length)\n",
    "\n",
    "\n",
    "# import os\n",
    "# from section_divide import _get_section\n",
    "# from classes.types import HoldType, JudgeType, NotesType\n",
    "# from classes.types.HoldType import HoldType\n",
    "\n",
    "# notes_file_paths = glob.glob(\"./../proseka/datas/*.json\")\n",
    "# notes_file_paths = [f for f in notes_file_paths if \"155\" in f or \"318\" in f]\n",
    "# for path_name in notes_file_paths:\n",
    "#     file_name = os.path.basename(path_name)\n",
    "#     sections = _get_section(path_name)\n",
    "#     id = int(path_name.split(\"song\")[1][:-5])\n",
    "#     labels = clustering_result_by_id[id]\n",
    "\n",
    "#     for j,section_j in enumerate(sections):\n",
    "#         # normal\n",
    "#         xs = np.array([note.x for note in section_j if note.type != NotesType.HOLD])\n",
    "#         ys = np.array([note.y for note in section_j if note.type != NotesType.HOLD], dtype=\"float64\")\n",
    "#         plt.scatter(xs,ys, color=\"b\")\n",
    "        \n",
    "#         hold_x = []\n",
    "#         hold_y = []\n",
    "#         for note in section_j:\n",
    "#             if note.type != NotesType.HOLD:\n",
    "#                 continue\n",
    "            \n",
    "#             hold_x.append(note.x)\n",
    "#             hold_y.append(note.y)\n",
    "            \n",
    "#             if note.hold_type == HoldType.END:\n",
    "#                 # plt.plot(hold_x, np.array(hold_y) - min_y, color=\"g\",  marker=\"o\")\n",
    "#                 plt.plot(hold_x, hold_y, color=\"g\",  marker=\"o\")\n",
    "#                 hold_x = []\n",
    "#                 hold_y = []\n",
    "#                 continue\n",
    "        \n",
    "#         if len(hold_x) != 0:\n",
    "#             plt.plot(hold_x, hold_y, color=\"g\",  marker=\"o\")\n",
    "#             hold_x = []\n",
    "#             hold_y = []\n",
    "            \n",
    "\n",
    "#         plt.xlim([-0.3,12.3])\n",
    "#         plt.savefig(f\"{cluster_labels_dir}/{id}/{labels[j]}/{j}.png\")\n",
    "#         plt.close()\n",
    "#         # if len(section_j) == 66:\n",
    "#         #     for note in section_j:\n",
    "#         #         print(note)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
